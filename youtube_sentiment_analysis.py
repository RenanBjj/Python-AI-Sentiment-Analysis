# -*- coding: utf-8 -*-
"""Youtube_Sentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17LtrwFYmTXIFOZdnGQh52pDxgfj_NQGM
"""

# pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client pandas torch transformers sentencepiece textblob matplotlib seaborn emoji

import pandas as pd
import os
from googleapiclient.discovery import build
import time
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
import emoji
from openai import OpenAI

# YouTube API Key
api_key = 'YOUR_API_KEY'

# Channel ID
channel_id = 'UCmXpqtgnZMtHc6bnAczlJaQ'

# Playlist com os vídeos desejados
upload_playlist_id = f'UU{channel_id[2:]}'

# OpenAI API Key
client = OpenAI(
    api_key='YOUR_API_KEY'
)

# Starts YouTube API
youtube = build("youtube", "v3", developerKey=api_key)

# Functions

# Functions to get videos and comments
def get_videos_and_comments(playlist_id, max_videos=50):
    all_data = []  # List to save video data and comments
    next_page_token = None  # Token for page control

    while True:
        # Request videos from a playlist
        request = youtube.playlistItems().list(
            part="snippet",
            playlistId=playlist_id,
            maxResults=max_videos,
            pageToken=next_page_token
        )
        response = request.execute()

        for item in response["items"]:
            video_id = item["snippet"]["resourceId"]["videoId"]
            title = item["snippet"]["title"]

            # Collect comments from videos
            comments = get_video_comments(video_id)

            for comment in comments:
                all_data.append({
                    "video_id": video_id,
                    "title": title,
                    "author": comment["author"],
                    "comment": comment["comment"]
                })

        next_page_token = response.get("nextPageToken")
        if not next_page_token:
            break

    return pd.DataFrame(all_data)


def get_video_comments(video_id):
    comments = []
    next_page_token = None

    while True:
        try:
            request = youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=100,
                pageToken=next_page_token
            )
            response = request.execute()

            for item in response.get("items", []):
                comment = item["snippet"]["topLevelComment"]["snippet"]
                comments.append({
                    "author": comment["authorDisplayName"],
                    "comment": comment["textDisplay"]
                })

            next_page_token = response.get("nextPageToken")
            if not next_page_token:
                break
        except Exception as e:
            print(f"Erro ao coletar comentários do vídeo {video_id}: {e}")
            break

    return comments


def load_sentiment_model(model_name):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    return tokenizer, model


# Function to analyze sentiment using * BERT *
def analyze_sentiment_bert(text):
    if isinstance(text, str):
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs)
            scores = F.softmax(outputs.logits, dim=1).numpy()[0]

        sentiment_labels = ["Very Negative", "Negative", "Neutral", "Positive", "Very Positive"]
        return sentiment_labels[scores.argmax()]
    return "Neutral"


# Function to analyze sentiment using * XLM-RoBERTa *
def analyze_sentiment_roberta(text):
    if isinstance(text, str):
        # Transform emoji into text
        text = emoji.demojize(text)

        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs)
            scores = F.softmax(outputs.logits, dim=1).numpy()[0]

        sentiment_labels = ["Negative", "Neutral", "Positive"]
        return sentiment_labels[scores.argmax()]
    return "Neutral"



# Function to send a comment to ChatGPT and get sentiment
def get_openai_sentiment_single(comment):
    prompt = f"""
    Classify the following comment into one of the categories:
    - Very Positive
    - Positive
    - Neutral
    - Negative
    - Very Negative

    Respond only with one of these categories, without explanations or additional formatting.

    Comment: "{comment}"
    """

    try:
        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an assistant specialized in sentiment analysis."},
                {"role": "user", "content": prompt}
            ]
        )

        response = completion.choices[0].message.content.strip()
        return response

    except Exception as e:
        print(f"Error processing comment: {e}")
        return "Neutral"  # Fallback to avoid errors

"""### Getting data from YouTube"""

# Collect video data and comments
df_comments = get_videos_and_comments(upload_playlist_id)

print(f"Total de vídeos/comentários coletados: {len(df_comments)}")
df_comments.head(10)

"""### Using BERT model to analyze sentiments"""

# Load the pre-trained BERT model for sentiment analysis
MODEL_NAME = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

# Apply sentiment analysis using BERT
df_comments["sentiment_bert"] = df_comments["comment"].apply(analyze_sentiment_bert)

# Count the sentiment distribution
sentiment_counts_bert = df_comments["sentiment_bert"].value_counts()
sentiment_percentages_bert = (sentiment_counts_bert / sentiment_counts_bert.sum()) * 100

# Create a bar chart
plt.figure(figsize=(8, 5))
ax = sns.barplot(x=sentiment_percentages_bert.index, y=sentiment_percentages_bert.values, palette="viridis")

for p in ax.patches:
    ax.annotate(f'{p.get_height():.1f}%',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=12, fontweight='bold')

plt.title("Sentiment Distribution in YouTube Comments - BERT")
plt.xlabel("Sentiment_BERT")
plt.ylabel("Percentage of Comments")
plt.ylim(0, 100)
plt.show()

"""### Using RoBERTa model to analyze sentiments"""

# Load the pre-trained XLM-RoBERTa model for sentiment analysis
MODEL_NAME = "cardiffnlp/twitter-xlm-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

# Apply sentiment analysis using XLM-RoBERTa
df_comments["sentiment_roberta"] = df_comments["comment"].apply(analyze_sentiment_roberta)

# Count the sentiment distribution
sentiment_counts_roberta = df_comments["sentiment_roberta"].value_counts()
sentiment_percentages_roberta = (sentiment_counts_roberta / sentiment_counts_roberta.sum()) * 100

# Create a bar chart
plt.figure(figsize=(8, 5))
ax = sns.barplot(x=sentiment_percentages_roberta.index, y=sentiment_percentages_roberta.values, palette="viridis")

for p in ax.patches:
    ax.annotate(f'{p.get_height():.1f}%',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=12, fontweight='bold')

plt.title("Sentiment Distribution in YouTube Comments - XLM-RoBERTa")
plt.xlabel("Sentiment_Roberta")
plt.ylabel("Percentage of Comments")
plt.ylim(0, 100)
plt.show()

"""### Using OpenAI API to analyze sentiments"""

# Get sentiment analisys created by ChatGPT

# OBS: chatgpt doesn't seem to respond well to many replies at once, so I'll send one at time to avoid inconsistencies

openai_sentiment = []

for comment in df_comments['comment']:
  single_sentiment = get_openai_sentiment_single(comment)
  openai_sentiment.append(single_sentiment)
  time.sleep(0.3) # little break

# Transform result list into a column
df_comments['sentiment_openai'] = openai_sentiment

# Count the sentiment distribution
sentiment_counts_openai = df_comments["sentiment_openai"].value_counts()
sentiment_percentages_openai = (sentiment_counts_openai / sentiment_counts_openai.sum()) * 100

# Create a bar chart
plt.figure(figsize=(8, 5))
ax = sns.barplot(x=sentiment_percentages_openai.index, y=sentiment_percentages_openai.values, palette="viridis")

for p in ax.patches:
    ax.annotate(f'{p.get_height():.1f}%',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=12, fontweight='bold')

plt.title("Sentiment Distribution in YouTube Comments - OpenAI")
plt.xlabel("Sentiment_OpenAI")
plt.ylabel("Percentage of Comments")
plt.ylim(0, 100)
plt.show()


print("Sentiment analysis completed using OpenAI API!")

# I decided to run the chatGPT again, and got a different answer, with shows a little bit of inconsistencies

"""### Comparing answers between models"""

# Create confusion matrix to compare BERT and OpenAI sentiments
conf_matrix = pd.crosstab(df_comments["sentiment_bert"], df_comments["sentiment_openai"])

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="Blues", linewidths=0.5, linecolor='gray')
plt.title("Confusion Matrix: BERT vs OpenAI Sentiments")
plt.xlabel("Sentiment OpenAI")
plt.ylabel("Sentiment BERT")
plt.show()

# Create confusion matrix to compare RoBERTa and OpenAI sentiments
conf_matrix = pd.crosstab(df_comments["sentiment_roberta"], df_comments["sentiment_openai"])

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="Blues", linewidths=0.5, linecolor='gray')
plt.title("Confusion Matrix: RoBERTa vs OpenAI Sentiments")
plt.xlabel("Sentiment OpenAI")
plt.ylabel("Sentiment RoBERTa")
plt.show()

# Here we can analyze the difference in opinions of some comments where the BERT model classified as negative and the other models as Positive / Very Positive

pd.set_option('display.max_colwidth', None)
df_comments[(df_comments['sentiment_bert'] == 'Negative') & ((df_comments['sentiment_roberta'] != 'Negative') | (df_comments['sentiment_openai'] != 'Negative'))]

# Adjust values for a fair comparison between models

df_mapped = df_comments.copy()
df_mapped["sentiment_openai"] = df_mapped["sentiment_openai"].replace({"Very Positive": "Positive", "Very Negative": "Negative"})
df_mapped["sentiment_bert"] = df_mapped["sentiment_bert"].replace({"Very Positive": "Positive", "Very Negative": "Negative"})

# Comparing agreement rate between BERT and OpenAI

bertvsopenai = (df_mapped["sentiment_bert"] == df_mapped["sentiment_openai"]).mean() * 100
robertavsopenai = (df_mapped["sentiment_roberta"] == df_mapped["sentiment_openai"]).mean() * 100
bertvsroberta = (df_mapped["sentiment_roberta"] == df_mapped["sentiment_bert"]).mean() * 100

print(f"Os modelos BERT e RoBERTa concordam em {bertvsroberta:.2f}% dos casos.")
print(f"Os modelos BERT e OpenAI concordam em {bertvsopenai:.2f}% dos casos.")
print(f"Os modelos RoBERTa e OpenAI concordam em {robertavsopenai:.2f}% dos casos.")

